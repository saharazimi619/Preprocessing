# Preprocessing
tokenizing sentences, tokenizing, stemming, lemmatization, cleaning the texts, creating the bag of words model, creating the TF_IDF model